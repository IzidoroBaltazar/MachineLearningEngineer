\documentclass[final,2p]{elsarticle}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{textcomp}
\usetikzlibrary{fit,arrows,calc,positioning}

\journal{Udacity}

\begin{document}

\begin{frontmatter}

\title{Capstone Project}

\author{Martin Pij{\'a}k}

\address{Bratislava, Slovakia}

\begin{abstract}
%% Text of abstract
This is project for using Machine Learning methods in futures trading.
\end{abstract}

\begin{keyword}
Machine Learning \sep{Trading} \sep{Udacity} \sep{Nanodegree}
%% keywords here, in the form: keyword \sep keyword

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
%% \linenumbers

%% main text
\section{Domain}

This project investigates possible futures trading strategies on Chicago Mercantile Exchange (CME) and Intercontinental Exchange (ICE) markets with Machine Learning methods. The goal is to find the trading strategy mostly based on the price, Commitment of Traders report (COT) and seasonality pattern. We will compare this trading strategy to commonly used investing approaches as returns of Nasdaq.

Following commodities were investigated:
\begin{itemize}
    \item \href{https://www.cmegroup.com/trading/metals/precious/gold.html}{Gold}
    \item \href{https://www.cmegroup.com/trading/agricultural/grain-and-oilseed/corn.html}{Corn}
    \item \href{https://www.cmegroup.com/trading/agricultural/softs/coffee.html}{Coffee}
\end{itemize}
\clearpage
\section{Datasets and Inputs}
%% \label{S:1}

I used data from \href{https://www.quandl.com/}{Quandl}. Data contains Open, High, Low, Close and volume (OHLCV) and commitment of traders (COT).
Continuous data was generated by taking contract with the highest volume for the trading day. For details please see \textbf{data-preparation.ipynb}.

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee.eps}}
\end{tabular}
\caption{Close price graphs}
\end{figure}

\section{ML Tools}
Following tools were used:
\begin{itemize}
    \item General ML tools
    \begin{itemize}
        \item \href{https://scikit-learn.org/}{Scikit}
    \end{itemize}
    \item Visualisation
        \begin{itemize}
            \item \href{https://matplotlib.org/}{matplotlib}
            \item \href{https://seaborn.pydata.org/}{seaborn}
        \end{itemize}
    \item Model building tools
        \begin{itemize}
            \item \href{https://keras.io/}{Keras}
            \item \href{https://www.tensorflow.org/}{TensorFlow (keras backend)}
            \item \href{https://www.h2o.ai/}{H2O}
            \item \href{https://lightgbm.readthedocs.io/}{LightGBM}
        \end{itemize}
\end{itemize}

\section{Data analysis}

In the proposal I wanted to investigate classification of volatility. Below is 95\% of daily volatility. 95\% was selected as a simulation of slippage.
General trading idea is to keep trade for a day. 
\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_gains_distribution.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_gains_distribution.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_gains_distribution.eps}}
\end{tabular}
\caption{Gains distribution}
\end{figure}

\subsection{Label Generation for classifier}
Labels were generated based on the trading target.
Target -1 is for short trade. 0 for no trade and 1 for long trade.

Threshold was chosen for different commodities differently based on the overall gains of the next day taking into account whole contract.

\begin{center}
\begin{tabular}{cc}
    \centering
    Commodity & Threshold (USD) \\
    \hline
    Gold & 500 \\
    Corn & 150 \\
    Coffee & 375 \\
\end{tabular}
\end{center}

\begin{gather}
    fee = 1.5 \\
    % t_{treshold} = 30 \\
    v_{volatility} = (\textup{close} - \textup{open})*\delta \textup{ where } \delta = 0.95 \\
    labels
    \begin{cases}
        | v_{volatility} | > t_{treshold} + fee \implies
        \begin{cases}
            v_{volatility} > 0 \implies 1 \textup{ (long)} \\
            v_{volatility} < 0 \implies -1 \textup{ (short)} \\
        \end{cases}\\
        | v_{volatility} | < t_{treshold} + fee \implies 0 \textup{ (no trade)}
    \end{cases}
\end{gather}
$\delta$ constant is used for simulating slippage.

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_target_distribution.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_target_distribution.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_target_distribution.eps}}
\end{tabular}
\caption{Labels distribution}
\end{figure}


I decided to have a look at regressor as well.

Based on the past 2 years of trading data (OHLCV and COT) classifier is deciding whether to trade or not.

Approach to training classifier/regressor and evaluation does not take into account that stop-loss can still make trade unsuccessfull.
From Machine Learning perspective the best approach is to have result of classification regression as close to the desired outcome as possible.

Stop-loss was selected so that 90\% of trades will be successfully executed (exited on close).

\begin{figure}[h!]
\centering
\begin{tabular}{cc}
\subfloat[Gold long]{\includegraphics[width = 2in]{figures/gold_stop_loss_long_trades.eps}} &
\subfloat[Gold short]{\includegraphics[width = 2in]{figures/gold_stop_loss_short_trades.eps}} \\
\subfloat[Corn long]{\includegraphics[width = 2in]{figures/corn_stop_loss_long_trades.eps}} &
\subfloat[Corn short]{\includegraphics[width = 2in]{figures/corn_stop_loss_short_trades.eps}} \\
\subfloat[Coffee long]{\includegraphics[width = 2in]{figures/coffee_stop_loss_long_trades.eps}} &
\subfloat[Coffee short]{\includegraphics[width = 2in]{figures/coffee_stop_loss_short_trades.eps}}
\end{tabular}
\caption{Stop Loss analysis counter position move}
\end{figure}

\begin{center}
\begin{tabular}{|c|c|c|}
    \hline
    Commodity & Long Stop Loss & Short Stop Loss \\
    \hline
    Gold & 1000 & 800 \\
    Corn & 300 & 300 \\
    Coffee & 800 & 800 \\
    \hline
\end{tabular}
\end{center}

\clearpage

\section{Feature Engineering}

We transformed last $2*252$ trading days into vector with $2520$ scalars.

In order to capture cyclicality I have transformed features as trading day of month, day of week and quarter of year into $sin$ and $cos$ values.
I am not sure whether this transformation grants subsequent PCA usage.
After adding data features vector now contains $2528$ scalars.

We transformed COT to reflect extreme in the index. COT 1 of industrials corresponds to the maximum of traders positions throughout the last 2 years.
Respectively 0 to minimum. We have included COT for commercial and industrial users from the last 8 weeks which is 16 values.
Training vector now contains $2528+16=2544$ values.

\subsection{PCA}
We try to capture about 80\% of variance of the data. For corn this corresponds to about $160$ components. For gold it is about $180$ components.
Case of coffee is very strange.

It is interesting Corn is best explained by PCA transformation. It is probably due to clear seasonal patterns in trading.

I am surprised that gold is better explained by PCA transformation than Coffee. I would expect that coffee has stronger seasonal trading patterns than gold because of the growth cycle. Maybe gold mining is subject to the weather in similar way as agricultural commodities. Gold is still mostly recycled and new production has limited impact on total amount of traded gold.

Possible explanations for lack of variance explanation by PCA for coffee:
\begin{itemize}
    \item corn traded on CME is mostly US produced with stable harvest season
    \item production of coffee is very unpredictable depending on the conditions of a given year
    \item there are multiple producers around the world (coffee is more of a global market with limited US production) with different harvest periods \href{https://driftaway.coffee/when-is-coffee-harvested/}{Coffee Harvest}
    \item important difference between coffee and corn is price per unit corn is much less efficient to transport
        \begin{itemize}
            \item 1 kg of corn is worth about $15\textcent$
            \item 1 kg of coffee is worth about $230\textcent$
        \end{itemize}
\end{itemize}
    
Based on the PCA variance graph I think that PCA transformation is not suitable for coffee. Information in components is growing linearly. If we don't see sharp increase of cumulative explained variance with few first components, then PCA transformation is not suitable. Therefore, coffee should not be considered for trading with PCA transformation. I will continue with coffee as well but based on this transformation I would not go ahead with trading unless I would find different transformation.
\begin{figure}[h]
    \centering
    \includegraphics{figures/pca.eps}
    \caption{PCA explained variance}
\end{figure}

\subsection{Outliers}
We checked dataset for outliers. I used IsolationForest from scikit with automatic outlier detection after PCA transformation.
Data was split into 3 sets training, validation and perfomance check.
Training set contains more that $1930$ samples. On this training set outlier detection was trained.
Following number of outliers were detected.

\begin{center}
\begin{tabular}{ccc}
    Commodity & Number of Inliers & Number of Outliers \\
    \hline
    Gold & 1927 & 6 \\
    Corn & 1915 & 15 \\
    Coffee & 1577 & 346 \\
\end{tabular}
\end{center}

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_outliers.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_outliers.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_outliers.eps}}
\end{tabular}
\caption{Outliers histogram}
\end{figure}

\section{Model Training}
\subsection{General strategy for training}
I tried the same model for all three commodities (gold, corn and coffee). I am looking for classifier with the same parameters for any commodity. This way I can be sure that I have found some general classification and good results are not just a coincidence.

Following exploration can be split into:

\begin{itemize}
    \item classificator
    \item regressor
\end{itemize}

\subsection{LightGBM regressor}

\subsubsection{Parameters}
I used following parameters to train LightGBM regressor. 

\begin{center}
\begin{tabular}{cc}
    Parameter & Value \\
    \hline
    num\_leaves & 40 \\
    objective & regression \\
    boosting & dart \\
    metric & l2 (mean squared error) \\
    estimators & 1000 \\
    learning\_rate & 0.001 \\
    num\_class & 1 \\
    max\_bin & 30 \\
    reg\_alpha & 5 \\
    reg\_lambda & 10 \\
    num\_round & 1000 \\
    early\_stopping\_rounds & 100 \\
\end{tabular}
\end{center}

\subsubsection{Training}
\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_lgb_training.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_lgb_training.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_lgb_training.eps}}
\end{tabular}
\caption{Training Error}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_lgb_kernel_density_residuals.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_lgb_kernel_density_residuals.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_lgb_kernel_density_residuals.eps}}
\end{tabular}
\caption{Residual Errors}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_lgb_actual_prediction.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_lgb_actual_prediction.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_lgb_actual_prediction.eps}}
\end{tabular}
\caption{Actual vs Prediction Scatter Plot}
\end{figure}

When determining trading performance of the LightGBM regressor I have chosen following trading thresholds.

\begin{center}
\begin{tabular}{cccc}
    Commodity & Trading threshold & Classifier threshold & Ratio \\
    Gold & 10 & 500 & $1\over{50}$ \\
    \\
    Corn & 10 & 150 & $1\over{15}$ \\
    \\
    Coffee & 10 & 375 & $1\over{38}$ \\
\end{tabular}
\end{center}

This part of trading process is somewhat unknown to me. Regressor is not predicting many days with big volatility. Predictions are much flatter than the actual volatility.

Based on the chosen threshold you can get some idea of how well is the regressor approximating actual volatility.

\clearpage

\subsubsection{Trading performance}

\begin{center}
\begin{tabular}{ccc}
    Commodity & Prediction/actual result correlation & Gains \\
    Gold & 0.079 & 42\% \\
    Corn & 0.078 & 5\% \\
    Coffee & -0.001 & 62\%
\end{tabular}
\end{center}

I evaluated trading performance on the year 2018. Below is account performance and histogram of trades.

\begin{figure}[h!]
\centering
\begin{tabular}{c}
\subfloat[Gold]{\includegraphics[width = 4in]{figures/gold_lgb_account.eps}} \\
\subfloat[Corn]{\includegraphics[width = 4in]{figures/corn_lgb_account.eps}} \\
\subfloat[Coffee]{\includegraphics[width = 4in]{figures/coffee_lgb_account.eps}}
\end{tabular}
\caption{Account performance}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_lgb_trade_histogram.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_lgb_trade_histogram.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_lgb_trade_histogram.eps}}
\end{tabular}
\caption{Trade histogram}
\end{figure}

\clearpage

\subsection{Neural Network Classication}

\subsubsection{Neural network design}
Neural network if fully connected with input layer, 2 hidden layers and output layer consisting of 3 neurons.
Output layer of 3 layers represent 3 categories $\{-1, 0, 1\}$.

\begin{center}
\begin{tabular}{cccccc}
    Input & input layer & \multicolumn{2}{c}{hidden layers} & output layer & output \\
    \hline
    200 & 32 & 16 & 8 & 3 & 3 \\
\end{tabular}
\end{center}

\subsubsection{Classifier Training}

In the training setup I was using training data with labels described above. If validation loss improved then new weights were saved.
Total number of epochs was $500$ and batch size $20$.

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_nn_clf_keras_accuracy.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_nn_clf_keras_accuracy.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_nn_clf_keras_accuracy.eps}}
\end{tabular}
\caption{Accuracy during training}
\end{figure}

%% \subsubsection{Cetegory precision score}
\subsubsection{Performance Evaluation}

Results below were obtained by running classifier on trading data from year 2018.

\begin{center}
    \begin{tabular}{cc}
        Commodity & Gains \\
        \hline
        Gold & 68 \% \\
        Corn & -40 \% \\
        Coffee & 0 \% (no trade) \\
    \end{tabular}
\end{center}

Precision Score:
\begin{center}
    \begin{tabular}{cccc}
        Commodity & Short & No trade & Long \\
        \hline
        Gold & 0. & 0.63 & 0.22 \\
        Corn & 0.12 & 0.64 & 0.15 \\
        Coffee & 0. & 0.39 & 0. \\
    \end{tabular}
\end{center}

\begin{figure}[h!]
    \centering
    \begin{tabular}{ccc}
        \subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_nn_cls_heatmap.eps}} &
        \subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_nn_cls_heatmap.eps}} & 
        \subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_nn_cls_heatmap.eps}}
    \end{tabular}
    \caption{Confusion Matrix}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_nn_cls_trade_histogram.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_nn_cls_trade_histogram.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_nn_cls_trade_histogram.eps}}
\end{tabular}
\caption{Trade histogram}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tabular}{c}
\subfloat[Gold]{\includegraphics[width = 4in]{figures/gold_nn_cls_account.eps}} \\
\subfloat[Corn]{\includegraphics[width = 4in]{figures/corn_nn_cls_account.eps}} \\
\subfloat[Coffee]{\includegraphics[width = 4in]{figures/coffee_nn_cls_account.eps}}
\end{tabular}
\caption{Trade histogram}
\end{figure}

\clearpage

\subsection{Neural Network Regressor}
\subsubsection{Neural Network Design}
Neural network if fully connected with input layer, 3 hidden layers and output layer consisting of 1 neuron.

\begin{center}
\begin{tabular}{ccccccc}
    Input & input layer & \multicolumn{3}{c}{hidden layers} & output layer & output \\
    \hline
    200 & 500 & 128 & 64 & 32 & 1 & 1 \\
\end{tabular}
\end{center}

\subsubsection{Regressor Training}

In the training setup I was using training data with labels described above. If validation loss improved then new weights were saved.
Total number of epochs was $1000$ and batch size $20$.

During training only in case of gold we can see improvement of loss on the validation set. For details see validation loss (MSE) below.

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Gold]{\includegraphics[width = 1.5in]{figures/gold_nn_reg_keras_mean_squared_error.eps}} &
\subfloat[Corn]{\includegraphics[width = 1.5in]{figures/corn_nn_reg_keras_mean_squared_error.eps}} &
\subfloat[Coffee]{\includegraphics[width = 1.5in]{figures/coffee_nn_reg_keras_mean_squared_error.eps}}
\end{tabular}
\caption{Mean Squared Error}
\end{figure}

\iffalse
Following schema is showing high level project pipeline.
\newline

\tikzstyle{block} = [draw, fill=blue!20, rectangle, 
    minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=blue!20, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\begin{tikzpicture}[node distance=4mm, >=latex',
 block/.style = {draw, rectangle, minimum height=10mm, minimum width=28mm,align=center},
rblock/.style = {draw, rectangle, rounded corners=0.5em, fill=blue!20,
                 minimum width={width("Reduce dimensionality")+2pt}},
tblock/.style = {draw, trapezium, minimum height=10mm, 
                 trapezium left angle=75, trapezium right angle=105, align=center},
                        ]

    \node [rblock]                      (gd)      {Gather Data};
    \node [rblock, below=of gd]         (input)   {Raw Input};
    \node [rblock, below=of input]      (fe)      {Feature Engineering};
    \node [rblock, below=of fe]         (rd)      {Reduce dimensionality};
    \node [rblock, left=of fe]          (label)   {Label Creation};
    \node [rblock, below=of rd]         (mf)      {Model Fit};
    \node [rblock, below=of mf]         (tst)     {Test};
    \node [rblock, below=of tst]        (perf)    {Evaluate Performance};

    \path[draw,->] (gd)         edge (input)
                   (input)      edge (fe)
                   (fe)         edge (rd)
                   (rd)         edge (mf)
                   (input)      -|   (label)
                   (label)      |-   (mf)
                   (mf)         edge (tst)
                   (tst)        edge (perf)
                    ;
\end{tikzpicture}
\newline
\newline
\fi

\section{Benchmarking Model}

I will use three benchmarks for my model

\begin{itemize}
    \item fixed percentage (bank/bond deposit comparison)
    \item Dow Jones Industrial Average performance
    \item mean reversal trading on given commodity markets
\end{itemize}

\section{Evaluation of trading}

Compare trading strategy to Nasdaq performance and mean reversal strategy.

\section{Conclusion}

Trading is a difficult ML problem. Out of three compared commodities gold, corn and coffee we were able to predict performance with gold. Other commodities behaved randomly with approximately 0 correlation to the actual volatility.

In the beginning of project, I was thinking of a classifier (short, no trade, long) because it is closer to the usage of model. I tested regressor as well. Regressor works better because there is more information. I tried different loss function when training regressor. I decided to use weighted MSE. This could be further modified for better function omitting errors below threshold.

Gold trading is the most capital intensive with very big stop losses (1000 long, 800 short). This can be problem for trading with $10 000\$$ account.

In case of corn and coffee the data was almost impossible to classify. I suspect more data transformation is needed to get better results.

\end{document}
